{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import glob as glob\n",
    "import dask.dataframe as dd\n",
    "import dask.distributed\n",
    "from dask import delayed\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name: variables\n",
    "Purpose: variables used to run different data sets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_2013_2014 = glob.glob(os.path.join(os.getcwd(), 'data2', 'green_tripdata_201[3-4]*.csv'))\n",
    "columnsGreen =[\n",
    "    'vendor_id', 'pickup_datetime', 'dropoff_datetime'\n",
    "    , 'Store_and_fwd_flag', 'RateCodeID', 'pickup_longitude', 'pickup_latitude'\n",
    "    , 'dropoff_longitude', 'dropoff_latitude', 'Passenger_count', 'Trip_distance'\n",
    "    , 'Fare_amount', 'Extra', 'MTA_tax', 'Tip_amount', 'Tolls_amount', 'Ehail_fee'\n",
    "    , 'Total_amount', 'Payment_type', 'Trip_type', 'extra1', 'extra2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name: ReadData\n",
    "Purpose: import Yellow Data to Dask DF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(globString, columns):\n",
    "\n",
    "    dtypes ={\n",
    "            'vendor_id': object               , 'passenger_count': object             , 'trip_distance': np.float64\n",
    "        , 'pickup_longitude': np.float64    , 'pickup_latitude': np.float64         , 'rate_code_id': object\n",
    "        , 'store_and_fwd_flag': object      , 'dropoff_longitude': np.float64       , 'dropoff_latitude': np.float64\n",
    "        , 'payment_type': object            , 'fare_amount': np.float64             , 'extra': np.float64\n",
    "        , 'mta_tax': np.float64             , 'tip_amount': np.float64              , 'tolls_amount': np.float64        \n",
    "        , 'total_amount': np.float64        , 'extra1': np.float64                  , 'extra2': np.float64\n",
    "        , 'extra3': np.float64              , 'improvement_surcharge': np.float64}\t\n",
    "\n",
    "    # Glob Import Strings for data files by file structure type\n",
    "    data = dd.read_csv( globString\n",
    "                        , header=0\n",
    "                        , na_values=['NA']\n",
    "                        , parse_dates=['pickup_datetime', 'dropoff_datetime']\n",
    "                        , usecols= ['pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude','dropoff_longitude', 'dropoff_latitude']\n",
    "                        #, infer_datetime_format=True\n",
    "                        , dtype= dtypes\n",
    "                        , names= columns)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name: joinLocIDBorough\n",
    "Purpose: map partition of Loc ID/Borough from Shape to lon lat in csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinLocIDBorough(data, longitude, latitude, newVar, ttype):\n",
    "\n",
    "    dataCopy = data[[longitude, latitude]].copy()\n",
    "    dataCopy[longitude] = dataCopy[longitude].fillna(value=0.)\n",
    "    dataCopy[latitude] = dataCopy[latitude].fillna(value=0.)\n",
    "\n",
    "\n",
    "    local_gdf = gpd.GeoDataFrame(dataCopy, crs={'init': 'epsg:4326'},geometry=[Point(xy) for xy in zip(dataCopy[longitude], dataCopy[latitude])])\n",
    "    local_gdf = gpd.sjoin(local_gdf, taxiShape, how='left', op='within')\n",
    "\n",
    "    # Determine Which Feature to add to DataFrame (Borough/Location ID)\n",
    "    if ttype== 'p':\n",
    "        return local_gdf.borough.rename(newVar)\n",
    "    elif ttype== 'd':\n",
    "        return local_gdf.LocationID.rename(newVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name: readShape\n",
    "Purpose: import Geometry/Weather Files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readShape():\n",
    "    global taxiShape\n",
    "\n",
    "    #Fetch Geometry Shape File\n",
    "    taxiShape = gpd.read_file(os.path.join(os.getcwd(), 'taxi_zones', 'taxi_zones.shp'))\n",
    "    taxiShape = taxiShape.to_crs({'init': 'epsg:4326'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name: filterData\n",
    "Purpose: maps long/lat to LocID/ Filters for MAN->JFK\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterData(data, folderName):\n",
    "\n",
    "    # sets borough for pickup location\n",
    "    data['pickup_borough'] = data.map_partitions(\n",
    "            joinLocIDBorough            # function to join lat/lon with Borough\n",
    "            , 'pickup_longitude'        # longitude param\n",
    "            , 'pickup_latitude'         # latitude param\n",
    "            , 'pickup_borough'          # new variable Name\n",
    "            , 'p'                       # denotes pickup\n",
    "            , meta=('pickup_borough', object))\n",
    "    # sets Location ID for destination\n",
    "    data['dropoff_location_id'] = data.map_partitions(\n",
    "            joinLocIDBorough            # function to join lat/lon with Loc ID\n",
    "            , 'dropoff_longitude'       # longitude param\n",
    "            , 'dropoff_latitude'        # latitude param\n",
    "            ,'dropoff_location_id'      # new variable Name\n",
    "            , 'd'                       # denotes dropoff\n",
    "            , meta=('dropoff_location_id', np.float64))\n",
    "\n",
    "    # Modify DateTime->Date/Filter Data to show only Manhattan->JFK\n",
    "    data = data[data['dropoff_location_id'] == np.float64(132)]\n",
    "    data = data[data['pickup_borough'] == 'Manhattan']\n",
    "    \n",
    "    # Output to Parquet for further processing\n",
    "    data.to_parquet(os.path.join(os.getcwd(),  'filtered', folderName), engine='pyarrow', compression='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name: ReadInData\n",
    "Purpose: import Modified Data/Weather\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadInData():\n",
    "\n",
    "    # Fetch Modified File from Parquet/Modify Date\n",
    "    filtered = dd.read_parquet(os.path.join(os.getcwd(), 'filtered', 'green_2013_2014'), engine='pyarrow')\n",
    "    filtered['pickup_datetime'] = filtered['pickup_datetime'].dt.date\n",
    "\n",
    "    # Fetch NOAA Weather Data File\n",
    "    weatherData = dd.read_csv(os.path.join(os.getcwd(), 'documentation', 'NOAA_Central_Park_data.csv')\n",
    "    , usecols=('DATE', 'PRCP'))\n",
    "    weatherData['DATE'] = dd.to_datetime(weatherData['DATE'])\n",
    "\n",
    "    return filtered, weatherData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name: filterData\n",
    "Purpose: filter to Man->JFK trips\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterData(filtered, weather):\n",
    "\n",
    "    #Create New Aggregated DataFrame\n",
    "    GroupedDates = filtered[['pickup_datetime']].groupby(['pickup_datetime']).size().reset_index()\n",
    "    GroupedDates['pickup_datetime'] = dd.to_datetime(GroupedDates['pickup_datetime'])\n",
    "    GroupedDates.columns = ['pickup_datetime', 'count']\n",
    "\n",
    "    #Join Aggregated DataFrame with Weather Data\n",
    "    newDF = dd.merge(GroupedDates, weather, left_on=['pickup_datetime'], right_on=['DATE'])\n",
    "    newDF.set_index('pickup_datetime')\n",
    "\n",
    "    return newDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name: analyzeData\n",
    "Purpose: find correlation between # of rides and weather\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeData(merged):\n",
    "    \n",
    "    # Seperate Data into 6 month periods (Winter/Summer)\n",
    "    winterSet = merged[merged['pickup_datetime'].dt.month.isin([10, 11, 12, 1, 2, 3])]\n",
    "    summerSet = merged[merged['pickup_datetime'].dt.month.isin([4, 5, 6, 7, 8, 9])]\n",
    "    \n",
    "    overallDataSet = merged[['count', 'PRCP']]\n",
    "    winterDataSet = winterSet[['count', 'PRCP']]\n",
    "    summerDataSet = summerSet[['count', 'PRCP']]\n",
    "\n",
    "    corr = overallDataSet.corr(method ='pearson')\n",
    "    wintercorr = winterDataSet.corr(method ='pearson')\n",
    "    summercorr = summerDataSet.corr(method ='pearson')\n",
    "    print('Overall Correlation Matrix:')\n",
    "    print(corr.head(2))\n",
    "    print('Winter Correlation Matrix:')\n",
    "    print(wintercorr.head(2))\n",
    "    print('Summer Correlation Matrix:')\n",
    "    print(summercorr.head(2))\n",
    "\n",
    "    fig = plt.figure(figsize = (30,30))\n",
    "    ax1 = fig.add_subplot(3,2,1)\n",
    "    ax2 = fig.add_subplot(3,2,2)\n",
    "    ax3 = fig.add_subplot(3,2,3)\n",
    "\n",
    "    sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), vmin=-0.1, vmax=0.1, cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax1)\n",
    "    sns.heatmap(wintercorr, mask=np.zeros_like(wintercorr, dtype=np.bool), vmin=-0.1, vmax=0.1, cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax2)\n",
    "    sns.heatmap(summercorr, mask=np.zeros_like(summercorr, dtype=np.bool), vmin=-0.1, vmax=0.1, cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Name: main\n",
    "Purpose: run above functions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(client):\n",
    "\n",
    "    # data processing \n",
    "    data = ReadData(green_2013_2014, columnsGreen)\n",
    "    readShape()\n",
    "    filterData(data, 'green_2013_2014')\n",
    "    \n",
    "    # data analysis\n",
    "    filtered, weatherData = ReadInData()\n",
    "    merged = filterData(filtered, weatherData)\n",
    "    analyzeData(merged)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = dask.distributed.Client()\n",
    "    main(client)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
